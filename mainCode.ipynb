{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af99aac",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "We want to train a simple feedforward neural network to calculate terminal velocity of a solid sphere of density $\\sigma$ and radius r, in a liquid of density $\\rho$ and viscosity $\\eta$.\n",
    "\n",
    "The formula for this terminal velocity classically is :\n",
    "$$V_{t} = \\frac{2r^{2}(\\rho - \\sigma)g}{9\\eta}$$\n",
    "\n",
    "Thus there will be 4 inputs into the neural network, densities $\\sigma$ and $\\rho$, viscosity $\\eta$ and radius r. And there will be one output corresponding to the terminal velocity.\n",
    "\n",
    "We will use above equation to simulate and create data to put into our neural network.\n",
    "\n",
    "\n",
    "### The twist\n",
    "\n",
    "We are only allowed to implement this neural network completely from scratch without any external libraries\n",
    "Only python and numpy allowed (and Matplotlib for plotting purposes)\n",
    "\n",
    "Additionally, I can only reference the 3b1b deep learning series for help, and chatgpt assisstance is limited to only conceptual doubts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc8da474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5435902",
   "metadata": {},
   "source": [
    "## Step 1: Creating the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61338c49",
   "metadata": {},
   "source": [
    "Chatgpt suggested these range of values for simulating the data\n",
    "\n",
    "- > r from 10^-4 to 5 * 10^-3 m\n",
    "- > $\\sigma$ from 500 to 8000 kg/m^3\n",
    "- > $\\rho$ from 700 to 1300 kg/m^3\n",
    "- > $\\eta$ from 0.001 to 2 Pa $\\cdot$ s\n",
    "\n",
    "We will consider 5k input data points into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2905d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPoints = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f72e74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed = 42)\n",
    "\n",
    "r = rng.uniform(low = 0.0001, high = 0.005, size = (dataPoints, 1))\n",
    "densSolid = rng.uniform(low = 500, high = 8000, size = (dataPoints, 1))\n",
    "densLiquid = rng.uniform(low = 700, high = 1300, size = (dataPoints, 1))\n",
    "viscosity = rng.uniform(low = 0.001, high = 2, size = (dataPoints, 1))\n",
    "\n",
    "data = np.hstack((r, densSolid, densLiquid, viscosity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c0a10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal(dats):\n",
    "    r, densL, densS, visc = dats\n",
    "    return (2*(r**2)*(densL - densS)*9.8)/(9*visc)\n",
    "\n",
    "v = np.apply_along_axis(terminal, 1, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dadf7",
   "metadata": {},
   "source": [
    "## Step 2: Train/test split and Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0defec",
   "metadata": {},
   "source": [
    "We shall do a 80/20 standard split\n",
    "\n",
    "We will use Z-score scaling or standardization to scale\n",
    "We will learn the means and stds from training data and apply same on testing data\n",
    "\n",
    "We will scale both inputs and outputs since thats necessary for neural networks, and we will unscale the outputs at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "810331d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = int((0.8) * data.shape[0])\n",
    "\n",
    "xTrain = data[:index, :]\n",
    "xTest = data[index: , :]\n",
    "yTrain = v[:index]\n",
    "yTest = v[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6964f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling inputs\n",
    "xMean = np.mean(xTrain, axis = 0)[np.newaxis, :]\n",
    "xStd = np.std(xTrain, axis = 0)[np.newaxis, :]\n",
    "\n",
    "xTrain = (xTrain - xMean)/xStd\n",
    "xTest = (xTest - xMean)/xStd\n",
    "\n",
    "#Scaling outputs\n",
    "yMean = np.mean(yTrain)\n",
    "yStd = np.std(yTrain)\n",
    "yTestOrig = yTest\n",
    "yTrainOrig = yTrain\n",
    "\n",
    "yTrain = (yTrain - yMean)/yStd\n",
    "yTest = (yTest - yMean)/yStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b25becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yScalingParams = np.array([yMean, yStd])\n",
    "\n",
    "np.save(\"./data/xTrain.npy\", xTrain)\n",
    "np.save(\"./data/xTest.npy\", xTest)\n",
    "np.save(\"./data/yTrain.npy\", yTrain)\n",
    "np.save(\"./data/yTest.npy\", yTest)\n",
    "np.save(\"./data/yTestOrig.npy\", yTestOrig)\n",
    "np.save(\"./data/yTrainOrig.npy\", yTrainOrig)\n",
    "np.save(\"./data/yScalingParams.npy\", yScalingParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55d28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
