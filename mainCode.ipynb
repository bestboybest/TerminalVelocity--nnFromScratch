{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af99aac",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "We want to train a simple feedforward neural network to calculate terminal velocity of a solid sphere of density $\\sigma$ and radius r, in a liquid of density $\\rho$ and viscosity $\\eta$.\n",
    "\n",
    "The formula for this terminal velocity classically is :\n",
    "$$V_{t} = \\frac{2r^{2}(\\rho - \\sigma)g}{9\\eta}$$\n",
    "\n",
    "Thus there will be 4 inputs into the neural network, densities $\\sigma$ and $\\rho$, viscosity $\\eta$ and radius r. And there will be one output corresponding to the terminal velocity.\n",
    "\n",
    "We will use above equation to simulate and create data to put into our neural network.\n",
    "\n",
    "\n",
    "### The twist\n",
    "\n",
    "We are only allowed to implement this neural network completely from scratch without any external libraries\n",
    "Only python and numpy allowed (and Matplotlib for plotting purposes)\n",
    "\n",
    "Additionally, I can only reference the 3b1b deep learning series for help, and chatgpt assisstance is limited to only conceptual doubts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dc8da474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5435902",
   "metadata": {},
   "source": [
    "## Step 1: Creating the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61338c49",
   "metadata": {},
   "source": [
    "Chatgpt suggested these range of values for simulating the data\n",
    "\n",
    "- > r from 10^-4 to 5 * 10^-3 m\n",
    "- > $\\sigma$ from 500 to 8000 kg/m^3\n",
    "- > $\\rho$ from 700 to 1300 kg/m^3\n",
    "- > $\\eta$ from 0.001 to 2 Pa $\\cdot$ s\n",
    "\n",
    "We will consider 5k input data points into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2905d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPoints = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f72e74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed = 42)\n",
    "\n",
    "r = rng.uniform(low = 0.0001, high = 0.005, size = (dataPoints, 1))\n",
    "densSolid = rng.uniform(low = 500, high = 8000, size = (dataPoints, 1))\n",
    "densLiquid = rng.uniform(low = 700, high = 1300, size = (dataPoints, 1))\n",
    "viscosity = rng.uniform(low = 0.001, high = 2, size = (dataPoints, 1))\n",
    "\n",
    "data = np.hstack((r, densSolid, densLiquid, viscosity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4c0a10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal(dats):\n",
    "    r, densL, densS, visc = dats\n",
    "    return (2*(r**2)*(densL - densS)*9.8)/(9*visc)\n",
    "\n",
    "v = np.apply_along_axis(terminal, 1, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dadf7",
   "metadata": {},
   "source": [
    "## Step 2: Train/test split and Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0defec",
   "metadata": {},
   "source": [
    "We shall do a 80/20 standard split\n",
    "\n",
    "We will use Z-score scaling or standardization to scale\n",
    "We will learn the means and stds from training data and apply same on testing data\n",
    "\n",
    "We will scale both inputs and outputs since thats necessary for neural networks, and we will unscale the outputs at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "810331d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = int((0.8) * data.shape[0])\n",
    "\n",
    "xTrain = data[:index, :]\n",
    "xTest = data[index: , :]\n",
    "yTrain = v[:index]\n",
    "yTest = v[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6964f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling inputs\n",
    "xMean = np.mean(xTrain, axis = 0)[np.newaxis, :]\n",
    "xStd = np.std(xTrain, axis = 0)[np.newaxis, :]\n",
    "\n",
    "xTrain = (xTrain - xMean)/xStd\n",
    "xTest = (xTest - xMean)/xStd\n",
    "\n",
    "#Scaling outputs\n",
    "yMean = np.mean(yTrain)\n",
    "yStd = np.std(yTrain)\n",
    "yTestOrig = yTest\n",
    "yTrainOrig = yTrain\n",
    "\n",
    "yTrain = (yTrain - yMean)/yStd\n",
    "yTest = (yTest - yMean)/yStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9b25becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yScalingParams = np.array([yMean, yStd])\n",
    "\n",
    "np.save(\"./data/xTrain.npy\", xTrain)\n",
    "np.save(\"./data/xTest.npy\", xTest)\n",
    "np.save(\"./data/yTrain.npy\", yTrain)\n",
    "np.save(\"./data/yTest.npy\", yTest)\n",
    "np.save(\"./data/yTestOrig.npy\", yTestOrig)\n",
    "np.save(\"./data/yTrainOrig.npy\", yTrainOrig)\n",
    "np.save(\"./data/yScalingParams.npy\", yScalingParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9616d3",
   "metadata": {},
   "source": [
    "## Step 3: Create Base Neural Network Architecture\n",
    "\n",
    "Each layer requires knowledge of previous layers, as thats how number of weights are decided\n",
    "\n",
    "The way we will construct this is using 2 classes, one for each layer, and another for the entire model on its own \n",
    "\n",
    "Each layer will take number of neurons from previous layer and construct its own weight and bias matrices, randomly of correct size, and we will create a forward function as well\n",
    "\n",
    "The activation function we will use in this neural network is the ReLU (Rectified Linear Unit)\n",
    "\n",
    "Important thing to note is that we shouldn't have an activation for our output layer, since our final output (terminal velocity) can and should take any value and shouldn't only be restricted to +ve values (which ReLU would do)\n",
    "\n",
    "Let's initially consider an architecture where inputs -> layer 1 -> layer 2 -> output\n",
    "\n",
    "Hidden layer 1 has 12 neurons, and hidden layer 2 has 8 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ee55d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    #Consider number of neurons in previous layer as x and number of layers in current layer as n\n",
    "    def __init__(self, x, n):\n",
    "        self.neurons = n\n",
    "        \n",
    "        self.weights = rng.uniform(low = -0.3, high = 0.3, size = (n, x))\n",
    "        self.biases = rng.uniform(low = -0.1, high = 0.1, size = (n, 1))\n",
    "    \n",
    "    def ReLU(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    #a is the matrix corresponding to the activations of previous layer\n",
    "    def forward(self, a, hidden):\n",
    "        return self.ReLU(np.matmul(self.weights, a) + self.biases) if hidden else np.matmul(self.weights, a) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f7a7a",
   "metadata": {},
   "source": [
    "The loss function we will be using is the MSE (Mean Squared Error) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "26517298",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1Neurons = 12\n",
    "l2Neurons = 8\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, Train, Test):\n",
    "        self.l1 = Layer(4, l1Neurons)\n",
    "        self.l2 = Layer(self.l1.neurons, l2Neurons)\n",
    "        self.output = Layer(self.l2.neurons, 1)\n",
    "\n",
    "        self.train = Train.T\n",
    "        self.test = Test.T\n",
    "    \n",
    "    def forwardpass(self, input):\n",
    "        a1 = self.l1.forward(input, 1)\n",
    "        a2 = self.l2.forward(a1, 1)\n",
    "        output = self.output.forward(a2, 0)\n",
    "        return output\n",
    "\n",
    "    def loss(self, pred, actual):\n",
    "        return np.mean(((actual - pred) ** 2))\n",
    "    \n",
    "    def testing(self):\n",
    "        loss = self.loss(self.forwardpass(self.train), self.test)\n",
    "        print(f\"The mean loss of our model is {loss}\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba0d53",
   "metadata": {},
   "source": [
    "### Experiment 1: \n",
    "\n",
    "We have implemented our basic framework to allow for forwardpass, so let's try our forwardpass on completely random values to set a baseline and to test our code thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0e37989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean loss of our model is 1.0090402535882859\n"
     ]
    }
   ],
   "source": [
    "nn1 = NN(xTrain, yTrain)\n",
    "loss = nn1.testing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
